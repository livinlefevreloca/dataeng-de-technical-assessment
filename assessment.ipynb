{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer - Technical Assessment\n",
    "\n",
    "In this section of the interview at Beyond Finance, you will be assessed on your ability to perform several Data Engineering tasks. To perform well on this task, you will demonstate competence in the following areas:\n",
    "\n",
    "* preprocessing data to prepare for a database load\n",
    "* understanding entity relationships in a database\n",
    "* merging data from different tables\n",
    "* filtering data to relevant subsets\n",
    "* calculating aggregations and descriptive statistics\n",
    "\n",
    "It will be pretty difficult to complete all questions in the allotted time. Your goal is not to speed through the answers, but to come up with answers that demonstrate your knowledge. It's more about your thought process and logic than getting the right answer or your code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "This exercise will be broken into 2 parts\n",
    "1. Data Processing\n",
    "2. Data Wrangling\n",
    "\n",
    "### Data Processing\n",
    "In this section you will take files from the ./raw_data/ subfolders, combine them into a single newline-delimited `json.gz` file per subfolder, and place that CSV file in a ./processed_data/ directory. You may have to do some light investigation into the data files to understand their file formats and delimiters\n",
    "\n",
    "**Example**\n",
    "\n",
    "Files\n",
    "- ./raw_data/tracks/tracks_0.csv\n",
    "- ./raw_data/tracks/tracks_1.json\n",
    "- ./raw_data/tracks/tracks_2.csv\n",
    "- etc... \n",
    "\n",
    "should be combined into a single file ./processed_data/tracks.json.gz\n",
    "\n",
    "**What we look for**\n",
    "\n",
    "- Can you handle all subfolders in a single pass over the raw data files?\n",
    "- How can you limit memory consumption? (hint `chunksize`)\n",
    "\n",
    "### Data Wrangling\n",
    "For this section, we'll pretend you loaded the raw data plus additional tables into a small SQLite database containing roughly a dozen tables. **We've provided this database for you so don't worry about loading it yourself**. If you are not familiar with the SQLite database, it uses a fairly complete and standard SQL syntax, though does not many advanced analytics functions. Consider it just a remote datastore for storing and retrieving data from. \n",
    "\n",
    "![](db-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing (40 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memory_profiler in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (0.58.0)\n",
      "Requirement already satisfied: psutil in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from memory_profiler) (5.8.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "!pip install memory_profiler\n",
    "import pandas as pd \n",
    "import os\n",
    "import shutil\n",
    "from io import StringIO\n",
    "import gzip\n",
    "\n",
    "\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing aggregated results out to processed_data/tracks.json.gz\n",
      "Processing file tracks_1.json\n",
      "Processing file tracks_0.csv\n",
      "Processing file tracks_2.csv\n",
      "Processing file tracks_4.csv\n",
      "Processing file tracks_3.json\n",
      "Writing aggregated results out to processed_data/playlist_track.json.gz\n",
      "Processing file playlist_track_0.csv\n",
      "Processing file playlist_track_2.csv\n",
      "Processing file playlist_track_4.csv\n",
      "Processing file playlist_track_1.json\n",
      "Processing file playlist_track_3.json\n",
      "Writing aggregated results out to processed_data/orders.json.gz\n",
      "Processing file orders_3.json\n",
      "Processing file orders_2.csv\n",
      "Processing file orders_0.csv\n",
      "Processing file orders_4.csv\n",
      "Processing file orders_1.json\n",
      "Writing aggregated results out to processed_data/track_facts.json.gz\n",
      "Processing file track_facts_0.csv\n",
      "Processing file track_facts_2.csv\n",
      "Processing file track_facts_3.json\n",
      "Processing file track_facts_4.csv\n",
      "Processing file track_facts_1.json\n",
      "peak memory: 95.29 MiB, increment: 19.59 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "\n",
    "# Some setup stuff to ensure idempotence of each run\n",
    "outdir = \"processed_data\"\n",
    "    \n",
    "if os.path.exists(outdir):\n",
    "    shutil.rmtree(outdir)\n",
    "\n",
    "os.makedirs(\"processed_data\")\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(\"./raw_data\"):\n",
    "    if dirs:\n",
    "        #initial iteration will be the root dir with the subdirectories in it...skip this.\n",
    "        continue\n",
    "    \n",
    "    outfile_name = os.path.join(outdir, f\"{os.path.basename(root)}.json.gz\")\n",
    "    print(f\"Writing aggregated results out to {outfile_name}\")\n",
    "    \n",
    "    # Open a gzip file in append-bytes mode to continually write  bytes to it\n",
    "    # the compression should occur when the bytes are written to file\n",
    "    # allowing gzip to maximize the compression ratio since it is compressing\n",
    "    # the full range of bytes not just each individual chunk\n",
    "    with gzip.open(outfile_name, \"ab\") as of:\n",
    "        for f in files:\n",
    "            print(f\"Processing file {f}\")\n",
    "            _, extenstion = os.path.splitext(f)\n",
    "            if extenstion == '.json':\n",
    "                # Open df in as TextIterator using chunksize option to read in 1000 lines at a time\n",
    "                # tried with 10000, 5000, 1000, 500 .\n",
    "                # 1000 seems to be the sweet spot usually having a \n",
    "                # max usage of 95MiB. Oddly it was better than 500 which was also slower.\n",
    "                df_iterator = pd.read_json(os.path.join(root, f), lines=True, chunksize=1000)\n",
    "                for df in df_iterator:\n",
    "                    # Instantiate a brand new buffer each loop. Using the same buffer and truncating each time \n",
    "                    # led to very slow iteration.\n",
    "                    buffer = StringIO()\n",
    "                    df.to_json(buffer, orient=\"records\", lines=True)\n",
    "                    \n",
    "                    # Seek to the begining of the buffer to prepare to read\n",
    "                    buffer.seek(0)\n",
    "                    \n",
    "                    # write out this chunk gzip file\n",
    "                    of.write(buffer.read().encode())\n",
    "\n",
    "            if extenstion == '.csv':\n",
    "                df_iterator = pd.read_csv(os.path.join(root, f), chunksize=1000)\n",
    "                for df in df_iterator:\n",
    "                    buffer = StringIO()\n",
    "                    df.to_json(buffer, orient=\"records\", lines=True)\n",
    "                    \n",
    "                    buffer.seek(0)\n",
    "                    of.write(buffer.read().encode())\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to ensure data was properly written to gzipped json files\n",
    "for root, _, files in os.walk(\"./processed_data\"):\n",
    "    for f in files:\n",
    "        pd.read_json(os.path.join(root, f), orient=\"records\", lines=True, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling (20 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-sql in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied: ipython>=1.0 in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from ipython-sql) (7.20.0)\n",
      "Requirement already satisfied: prettytable<1 in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from ipython-sql) (0.7.2)\n",
      "Requirement already satisfied: six in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from ipython-sql) (1.15.0)\n",
      "Requirement already satisfied: sqlparse in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from ipython-sql) (0.4.1)\n",
      "Requirement already satisfied: ipython-genutils>=0.1.0 in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from ipython-sql) (0.2.0)\n",
      "Requirement already satisfied: sqlalchemy>=0.6.7 in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from ipython-sql) (1.3.23)\n",
      "Requirement already satisfied: pygments in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (2.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (3.0.16)\n",
      "Requirement already satisfied: pickleshare in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (0.7.5)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (0.1.2)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (4.8.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (5.0.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (41.2.0)\n",
      "Requirement already satisfied: backcall in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (0.18.0)\n",
      "Requirement already satisfied: decorator in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (4.4.2)\n",
      "Requirement already satisfied: wcwidth in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=1.0->ipython-sql) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=1.0->ipython-sql) (0.7.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/adam.lefevre/.pyenv/versions/3.7.6/envs/beyond-finance/lib/python3.7/site-packages (from jedi>=0.16->ipython>=1.0->ipython-sql) (0.8.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipython-sql\n",
    "%load_ext sql \n",
    "%sql sqlite:///db/sqlite/chinook.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect(\"db/sqlite/chinook.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How many different customers are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * sqlite:///db/sqlite/chinook.db\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>COUNT(CustomerId)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>59</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(59,)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "-- CustomerId is primary key so no need for distinct\n",
    "SELECT COUNT(CustomerId) FROM customers;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How long is the longest track in minutes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * sqlite:///db/sqlite/chinook.db\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>minutes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>88.11588333333334</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(88.11588333333334,)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT (CAST(MAX(Milliseconds) AS DOUBLE PRECISION) / 1000) / 60 AS minutes FROM tracks;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Which genre has the shortest average track length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * sqlite:///db/sqlite/chinook.db\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Rock And Roll</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('Rock And Roll',)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "-- Select the top record of the list of genres sorted by their average track length\n",
    "SELECT g.name\n",
    "FROM tracks t\n",
    "JOIN genres g\n",
    "ON t.GenreId = g.GenreId\n",
    "GROUP BY t.GenreId\n",
    "ORDER BY AVG(Milliseconds) ASC\n",
    "LIMIT 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Which artist shows up in the most playlists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * sqlite:///db/sqlite/chinook.db\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>Name</th>\n",
       "        <th>playlists</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Eugene Ormandy</td>\n",
       "        <td>7</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('Eugene Ormandy', 7)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "-- Join playlists to artists and group by artists name then orderby distinct playlistIds and take the top\n",
    "\n",
    "SELECT ar.Name, COUNT(distinct p.PlayListId) as playlists\n",
    "FROM playlists p\n",
    "JOIN playlist_track pt\n",
    "    ON p.PlaylistId = pt.PlaylistId\n",
    "JOIN tracks t\n",
    "    ON t.TrackId = pt.TrackId\n",
    "JOIN albums ab\n",
    "    ON ab.AlbumId = t.AlbumId\n",
    "JOIN artists ar\n",
    "    ON ar.ArtistId = ab.ArtistId\n",
    "GROUP BY ar.Name\n",
    "ORDER BY COUNT(distinct p.PlayListId) DESC\n",
    "LIMIT 1;\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What was the most popular album among these customers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * sqlite:///db/sqlite/chinook.db\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>Title</th>\n",
       "        <th>tracks_from_album_sold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Minha Historia</td>\n",
       "        <td>27</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('Minha Historia', 27)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "-- Join invoices to albums and group by album title order by number of invoice lines for each album. Take the top.\n",
    "\n",
    "SELECT a.Title, COUNT(distinct ii.InvoiceLineId) tracks_from_album_sold\n",
    "FROM invoices i\n",
    "JOIN invoice_items ii\n",
    "    ON i.InvoiceId = ii.InvoiceId\n",
    "JOIN tracks t\n",
    "    ON ii.TrackId = t.TrackId\n",
    "JOIN albums a\n",
    "    ON a.AlbumId = t.AlbumId\n",
    "GROUP BY a.Title\n",
    "ORDER BY COUNT(distinct ii.InvoiceLineId) DESC\n",
    "LIMIT 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
